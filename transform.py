import os
import os.path
import re


def ret_op( line ):
	return line.split()[0]


'''
Function join_grams:

Input: 
 		1)x : (int) lineNumber 
	 	2)arr : list of (string)lines

Returns: 
	 	4gram of the form {opCode1,opCode2,opCode3,opCode4}
'''
def join_grams( x, arr ):
	if( arr[x] ):
		token1 = ret_op( line = arr[x] )
		if( x+3 < len(arr) ):	
			token2 = ret_op( line = arr[x+1] )
			token3 = ret_op( line = arr[x+2] )
			token4 = ret_op( line = arr[x+3] )
			gram = "{" + token1 + "," + token2 + "," + token3 + "," + token4 + "}"
			return gram
		else :
			return 69

'''
Function cleanASM:

Input: 
 		1)filename : string 

Returns: 
	 	A list of lines of the form [(opCode Arguments)]
'''
def cleanASM(filename):
	
	filename = 'ASM/' + filename + '.asm'
	parsedLines = []
	filteredLines = []

	# Main regexp
	filterPattern = re.compile(
		"^(\.text|\.icode):([0-9a-zA-Z]{8,8})"					#Matches expresion of the form .text || .code : xxxxxxxx  where x can be a digit or char
		"(\s[0-9a-zA-Z]{2,2}[\+]{0,1})+(\t*)(\s*)"			   	#Starts with space followed by n pair of letter|digit which might be followed by a '+' 
		"(?!\s*;)(.*)"                                         	#match only if not followed by spaces and then ';' 
	)
	
	# Inline comments regexp
	commentFilterPattern = re.compile(
		"(.*)(;.*)"
	)
	
	
	inpLines = open(filename).readlines()
	count = 0
	for line in inpLines:
		match = filterPattern.search(line)
		if match:
			line = match.group(6)
			if(match.group(1) == '.text'):
				count += 1
			if line and not line.startswith(("ord","unk","stru","ru_","unicode","db ","CC ","00","dd","dword","off_","var","arg","byte","public","assume","word","_","align"," ","\n",'?','dw ')):
				filteredLines.append(line)

	# Gets rid of comments in the filtered line if any
	for line in filteredLines:
		match = commentFilterPattern.search(line)
		if match:
			line = match.group(1)
			parsedLines.append(line)
		else:
			parsedLines.append(line)	
	return parsedLines,count
pass


'''
Function tokenizer:

Input: 
 		1)line : string 

Returns: 
	 	A dictonary of the form:
	 	{type:Instruction|Procedure|Label , name:opCodeName|ProcedureName|LabelName , operands:Instruction Operands}
'''
def tokenizer(line):
	token = None
	instructionPattern = re.compile(
			"^(\w+)(?!\s+endp)(\s+.*)"			#beginning of line matches atleast 1 char and is not followed by spaces and endp but followed by spaces and anythin
	)
	procPattern = re.compile(
		"(sub_(\d*|\w*)*|\w*)\s*(proc\s+near)"
	)
	labelPattern = re.compile(
		"^((\w+_.*)|(_\w+)):"
	)
	match = labelPattern.search(line)
	if match:
		labelName = match.group(1)
		token = {}
		token["type"] = "LABEL"
		token["name"] = labelName
	else:
		match = procPattern.search(line)
		if match:
			procName = match.group(1)
			token = {}
			token["type"] = "PROCEDURE"
			token["name"] = procName
		else :
			match = instructionPattern.search(line)
			if match:
				instruction = match.group(1)
				operands = match.group(2)
				operands = [x.strip() for x in operands.split(',')]  #gets rid of extra spaces

				token = {}
				token["type"] = "INSTRUCTION"
				token["name"] = instruction
				token["operands"] = operands
	return token
pass



	
